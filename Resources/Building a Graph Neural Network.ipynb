{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Building a Graph Neural Network from Scratch\n",
    "> Author: Nathan Albin (albin@k-state.edu)\\\n",
    "> Updated: 2024-05-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "import torch_geometric.nn as tgnn\n",
    "\n",
    "from modified_cora import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Introduction\n",
    "\n",
    "We are building this notebook in class as a way to see how graph neural networks are built step-by-step. Although we will eventually use the PyG library, my hope is that building a network by hand will help remove some of the mystery surrounding how these networks operate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# The data\n",
    "\n",
    "The next few cells load the (modified) Cora dataset and print a few rows of each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "citation data is in cora.tgz\n",
      "\n"
     ]
    }
   ],
   "source": [
    "content_df, cites_df = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>word_0000</th>\n",
       "      <th>word_0001</th>\n",
       "      <th>word_0002</th>\n",
       "      <th>word_0003</th>\n",
       "      <th>word_0004</th>\n",
       "      <th>word_0005</th>\n",
       "      <th>word_0006</th>\n",
       "      <th>word_0007</th>\n",
       "      <th>word_0008</th>\n",
       "      <th>...</th>\n",
       "      <th>word_1424</th>\n",
       "      <th>word_1425</th>\n",
       "      <th>word_1426</th>\n",
       "      <th>word_1427</th>\n",
       "      <th>word_1428</th>\n",
       "      <th>word_1429</th>\n",
       "      <th>word_1430</th>\n",
       "      <th>word_1431</th>\n",
       "      <th>word_1432</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31336</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Neural_Networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1061127</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Rule_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1106406</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13195</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Reinforcement_Learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37879</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Probabilistic_Methods</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1435 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   paper_id  word_0000  word_0001  word_0002  word_0003  word_0004  word_0005  \\\n",
       "0     31336          0          0          0          0          0          0   \n",
       "1   1061127          0          0          0          0          0          0   \n",
       "2   1106406          0          0          0          0          0          0   \n",
       "3     13195          0          0          0          0          0          0   \n",
       "4     37879          0          0          0          0          0          0   \n",
       "\n",
       "   word_0006  word_0007  word_0008  ...  word_1424  word_1425  word_1426  \\\n",
       "0          0          0          0  ...          0          0          1   \n",
       "1          0          0          0  ...          0          1          0   \n",
       "2          0          0          0  ...          0          0          0   \n",
       "3          0          0          0  ...          0          0          0   \n",
       "4          0          0          0  ...          0          0          0   \n",
       "\n",
       "   word_1427  word_1428  word_1429  word_1430  word_1431  word_1432  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "2          0          0          0          0          0          0   \n",
       "3          0          0          0          0          0          0   \n",
       "4          0          0          0          0          0          0   \n",
       "\n",
       "                    label  \n",
       "0         Neural_Networks  \n",
       "1           Rule_Learning  \n",
       "2  Reinforcement_Learning  \n",
       "3  Reinforcement_Learning  \n",
       "4   Probabilistic_Methods  \n",
       "\n",
       "[5 rows x 1435 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35</td>\n",
       "      <td>1033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>103482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35</td>\n",
       "      <td>103515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>1050679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>1103960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target   source\n",
       "0      35     1033\n",
       "1      35   103482\n",
       "2      35   103515\n",
       "3      35  1050679\n",
       "4      35  1103960"
      ]
     },
     "execution_count": 6,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cites_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Unpacking the data\n",
    "\n",
    "Now we unpack the data into more efficient data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Nodes and edges\n",
    "The next code cell builds lists for the nodes and edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2485 nodes.\n",
      "  31336, 1061127, 1106406, 13195, 37879, ...\n",
      "\n",
      "There are 5209 edges.\n",
      "  [35, 1033], [35, 103482], [35, 103515], [35, 1050679], [35, 1103960], ...\n"
     ]
    }
   ],
   "source": [
    "# list of nodes\n",
    "nodes = content_df['paper_id'].to_list()\n",
    "\n",
    "# list of edges\n",
    "edges = cites_df.values.tolist()\n",
    "\n",
    "print(f'There are {len(nodes)} nodes.')\n",
    "print('  ' + ', '.join([str(v) for v in nodes[:5]]) + ', ...')\n",
    "print()\n",
    "print(f'There are {len(edges)} edges.')\n",
    "print('  ' + ', '.join([str(e) for e in edges[:5]]) + ', ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Labels\n",
    "\n",
    "Each node $u\\in V$ has an associated label $\\ell_u\\in L$, where $L$ is the label set. (I've updated to code to use Numpy, as pointed out in class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 unique labels.\n",
      "  0 -> Case_Based\n",
      "  1 -> Genetic_Algorithms\n",
      "  2 -> Neural_Networks\n",
      "  3 -> Probabilistic_Methods\n",
      "  4 -> Reinforcement_Learning\n",
      "  5 -> Rule_Learning\n",
      "  6 -> Theory\n"
     ]
    }
   ],
   "source": [
    "# find the unique labels\n",
    "label_set = np.unique(content_df['label'])\n",
    "\n",
    "print(f'There are {len(label_set)} unique labels.')\n",
    "for i,l in enumerate(label_set):\n",
    "    print(f'  {i} -> {l}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Instead of storing the text label for each node, we decided to store the corresponding index. We can do that by creating a dictionary that maps from each label to its corresponding index. (Essentially inverting the index-to-label map implicit in the label_set list.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label map:\n",
      "  Case_Based             -> 0\n",
      "  Genetic_Algorithms     -> 1\n",
      "  Neural_Networks        -> 2\n",
      "  Probabilistic_Methods  -> 3\n",
      "  Reinforcement_Learning -> 4\n",
      "  Rule_Learning          -> 5\n",
      "  Theory                 -> 6\n"
     ]
    }
   ],
   "source": [
    "# dictionary : label -> index\n",
    "label_map = { l : i for i,l in enumerate(label_set)}\n",
    "\n",
    "print('Label map:')\n",
    "for l in label_map:\n",
    "    print(f'  {l:22s} -> {label_map[l]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, we could make a dictionary mapping nodes to label (index), as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 31336 has label index 2.\n",
      "Node 1061127 has label index 5.\n",
      "Node 1106406 has label index 4.\n",
      "Node 13195 has label index 4.\n",
      "Node 37879 has label index 3.\n"
     ]
    }
   ],
   "source": [
    "# dictionary : node -> label index\n",
    "labels = {u : x for u,x in zip(nodes, content_df['label'].map(lambda l : label_map[l]).values)}\n",
    "\n",
    "for u in nodes[:5]:\n",
    "    print(f'Node {u} has label index {labels[u]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "However, for our purposes (vector operations), it is more efficient to store the labels as a vector $y\\in\\mathbb{R}^n$ and use a dictionary to match the nodes to the corresponding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 31336 has label index 2.\n",
      "Node 1061127 has label index 5.\n",
      "Node 1106406 has label index 4.\n",
      "Node 13195 has label index 4.\n",
      "Node 37879 has label index 3.\n"
     ]
    }
   ],
   "source": [
    "# dictionary : node -> row index in feature matrix\n",
    "node_map = {u : i for i,u in enumerate(nodes)}\n",
    "\n",
    "# label vector\n",
    "y = content_df['label'].map(lambda l : label_map[l]).values\n",
    "\n",
    "for u in nodes[:5]:\n",
    "    print(f'Node {u} has label index {y[node_map[u]]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Features\n",
    "\n",
    "Next, we need to keep track of the feature vector $x_u$ for each node $u\\in V$. Our first iteration was to use a dictionary mapping $V$ to $\\mathbb{R}^d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 31336 contains 20 of the key words.\n",
      "Node 1061127 contains 17 of the key words.\n",
      "Node 1106406 contains 22 of the key words.\n",
      "Node 13195 contains 21 of the key words.\n",
      "Node 37879 contains 23 of the key words.\n"
     ]
    }
   ],
   "source": [
    "# dictionary : node -> feature vector\n",
    "features = {u : x for u,x in zip(nodes, content_df.iloc[:,1:-1].values)}\n",
    "\n",
    "for u in nodes[:5]:\n",
    "    print(f'Node {u} contains {sum(features[u])} of the key words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Again, this is not particularly efficient for some of the things we will need to do (e.g., matrix multiplication). So, instead, we'll store the feature vectors as rows in a $n\\times d$ matrix and use the node map to index into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 31336 maps to index 0 and contains 20.0 of the key words.\n",
      "Node 1061127 maps to index 1 and contains 17.0 of the key words.\n",
      "Node 1106406 maps to index 2 and contains 22.0 of the key words.\n",
      "Node 13195 maps to index 3 and contains 21.0 of the key words.\n",
      "Node 37879 maps to index 4 and contains 23.0 of the key words.\n"
     ]
    }
   ],
   "source": [
    "# feature matrix\n",
    "X = content_df.iloc[:,1:-1].values.astype(float)     # converted to float in anticpation of using pytorch\n",
    "\n",
    "for u in nodes[:5]:\n",
    "    print(f'Node {u} maps to index {node_map[u]} and contains {sum(X[node_map[u],:])} of the key words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GCN by hand\n",
    "\n",
    "In this section, we'll derive the graph convolutional network (GCN) from the following paper.\n",
    "\n",
    "> Kipf, T.N., Welling, M., 2017. Semi-Supervised Classification with Graph Convolutional Networks. https://doi.org/10.48550/arXiv.1609.02907\n",
    "\n",
    "The basic idea is to perform linear transformations of the form\n",
    "\n",
    "$$\n",
    "f(X)_u = W^T\\left(\\sum_{v\\in\\mathcal{N}_u}\\frac{1}{\\sqrt{d_ud_v}}x_v\\right)+b.\n",
    "$$\n",
    "\n",
    "A few important points:\n",
    "- The graph is modified to include self-loops. (Essentially, this replaces the adjacency matrix $A$ by $A+I$.)\n",
    "- $d_u$ and $d_v$ are the degrees of the vertices $u$ and $v$ respectively (including counting the new self-loop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Storing the data in pytorch tensors\n",
    "\n",
    "For this example, I subtracted the average from each feature vector so that each feature vector has average 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# pytorch feature tensor\n",
    "X_pt = torch.from_numpy(X - np.average(X,axis=1)[:,None])\n",
    "\n",
    "# pytorch label tensor\n",
    "y_pt = torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## First (slow) attempt\n",
    "\n",
    "Here's a fairly straightforward implementation of the formula. The weigths, $W$, and biases, $b$, are included by attaching a PyTorch <tt>Linear</tt> layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class SlowGCNLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, nodes, edges, in_channels, out_channels):\n",
    "        \n",
    "        # call the nn.Module initialization routine\n",
    "        super().__init__()\n",
    "        \n",
    "        # attach basic data\n",
    "        self.nodes = nodes\n",
    "        self.edges = edges\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        # dictionary : node_id -> index\n",
    "        self.node_map = {u : i for i,u in enumerate(nodes)}\n",
    "        \n",
    "        # linear operator holding the weights and biases\n",
    "        self.L = nn.Linear(in_channels, out_channels)\n",
    "        \n",
    "        # initialize a neighbor dictionary with self-loops\n",
    "        self.neighbors = { u : set([u]) for u in nodes}\n",
    "        \n",
    "        # add other neighbors from the edges list\n",
    "        for u,v in edges:\n",
    "            self.neighbors[u].add(v)\n",
    "            self.neighbors[v].add(u)\n",
    "\n",
    "    def degree(self, u):\n",
    "        return len(self.neighbors[u])\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \n",
    "        # tensor to accumulate the sum over neighbors\n",
    "        fX = torch.zeros_like(X)\n",
    "        \n",
    "        # loop over all nodes\n",
    "        for u in self.nodes:\n",
    "            \n",
    "            # gather node index and degree\n",
    "            u_ind = self.node_map[u]\n",
    "            u_deg = self.degree(u)\n",
    "            \n",
    "            # loop over all neighbors\n",
    "            for v in self.neighbors[u]:\n",
    "                \n",
    "                # gather node index and degree\n",
    "                v_ind = self.node_map[v]\n",
    "                v_deg = self.degree(v)\n",
    "                \n",
    "                # update u's row in the summation\n",
    "                fX[u_ind,:] += X[v_ind,:]/np.sqrt(u_deg*v_deg)\n",
    "\n",
    "        # apply the linear operator to the sum and return\n",
    "        return self.L(fX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here's an example that creates a GCN layer with 10 output channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2485, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_channels = X.shape[1]\n",
    "out_channels = 10\n",
    "L1 = SlowGCNLayer(nodes, edges, in_channels, out_channels)\n",
    "\n",
    "L1(X_pt).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<tt>for</tt> loops in Python tend to be slow. Nested <tt>for</tt> loops are even slower. Knowing that, we shouldn't expect our current GCN implementation to be particuarly efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "858 ms Â± 72.7 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit L1(X_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Second (faster) attempt\n",
    "\n",
    "As discussed in class, the operation inside the summation is equivalent to left-multiplication of the data by the matrix $T=D^{-\\frac{1}{2}}AD^{-\\frac{1}{2}}$. Linear algebra operations tend to be faster in Python libraries like Numpy and PyTorch because these libraries include compiled implementations that are much more efficient than Python's for loops. The code cell below sets up the $T$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_236/1417422144.py:21: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
      "  T_pt = torch.from_numpy(T).to_sparse_csr()\n"
     ]
    }
   ],
   "source": [
    "# dimension of the adjacency matrix\n",
    "num_nodes = len(nodes)\n",
    "\n",
    "# initialize with self-loops\n",
    "A = np.eye(num_nodes)\n",
    "\n",
    "# add entries based on edge list\n",
    "for u,v in edges:\n",
    "    u_ind = node_map[u]\n",
    "    v_ind = node_map[v]\n",
    "    A[u_ind,v_ind] = 1\n",
    "    A[v_ind,u_ind] = 1\n",
    "\n",
    "# create the diagonal matrix D^{-1/2} \n",
    "DD = np.diag(1/np.sqrt(A.sum(axis=1)))\n",
    "\n",
    "# create the T matrix\n",
    "T = DD@A@DD\n",
    "\n",
    "# convert to a sparse PyTorch tensor\n",
    "T_pt = torch.from_numpy(T).to_sparse_csr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here's an updated implementation of the GCN layer using the $T$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "class GCNLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, T, in_channels, out_channels):\n",
    "        \n",
    "        # call the nn.Module initialization routine\n",
    "        super().__init__()\n",
    "        \n",
    "        # attach basic data\n",
    "        self.T = T\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        # linear operator holding the weights and biases\n",
    "        self.L = nn.Linear(in_channels, out_channels)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        # apply T and the linear operator\n",
    "        return self.L(self.T@X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To compare the two implementations, we can create a member of the new class and copy the weights and biases from the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2 = GCNLayer(T_pt, in_channels, out_channels)\n",
    "\n",
    "L2.load_state_dict(L1.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As a simple test of our code, we can check if the two layers give the same output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.270169654765694e-15\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(torch.norm(L1(X_pt) - L2(X_pt)).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can also compare the evaluation time required to the previous implementation. The implementation using matrix multiplication should be 10-20 times faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.2 ms Â± 2.78 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit L2(X_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Putting the 'N' in GCN\n",
    "\n",
    "Now it's time to build a network out of these layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "hidden_channels = 256\n",
    "hidden_layers = 4\n",
    "out_channels = len(label_set)\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    \n",
    "    def __init__(self, T, in_channels, out_channels, hidden_channels, hidden_layers):\n",
    "        \n",
    "        # super-class initializer\n",
    "        super().__init__()\n",
    "        \n",
    "        # create a module list for the layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # first hidden layer : in_channels -> hidden_channels\n",
    "        self.layers.append(GCNLayer(T, in_channels, hidden_channels))\n",
    "        \n",
    "        # remaining hidden layers : hidden_channels -> hidden_channels\n",
    "        for i in range(hidden_layers-1):\n",
    "            self.layers.append(GCNLayer(T, hidden_channels, hidden_channels))\n",
    "            \n",
    "        # output layer : hidden_channels -> out_channels\n",
    "        self.layers.append(GCNLayer(T, hidden_channels, out_channels))\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        dropout = nn.functional.dropout\n",
    "        \n",
    "        l = self.layers[0]\n",
    "        X = l(X).relu()\n",
    "        X = dropout(X, 0.75, training=self.training)\n",
    "        \n",
    "        for l in self.layers[1:-1]:\n",
    "            X = X + l(X).relu()\n",
    "\n",
    "        X = dropout(X, 0.75, training=self.training)\n",
    "        l = self.layers[-1]\n",
    "        return l(X)\n",
    "    \n",
    "model = GCN(T_pt, in_channels, out_channels, hidden_channels, hidden_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training and testing data\n",
    "\n",
    "Here we split the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 1485\n",
      "test:  1000\n"
     ]
    }
   ],
   "source": [
    "num_nodes = len(nodes)\n",
    "train_ind, test_ind = train_test_split(range(num_nodes), test_size=1000, stratify=y)\n",
    "\n",
    "print(f'train: {len(train_ind)}')\n",
    "print(f'test:  {len(test_ind)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch     0  train loss  1.29974e-03  train acc  17.98%  test acc  18.70%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    10  train loss  1.22226e-03  train acc  29.23%  test acc  29.20%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    20  train loss  1.12232e-03  train acc  29.23%  test acc  29.20%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    30  train loss  9.50574e-04  train acc  65.25%  test acc  62.60%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    40  train loss  6.82115e-04  train acc  76.50%  test acc  75.10%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    50  train loss  4.50956e-04  train acc  82.69%  test acc  81.60%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    60  train loss  3.34537e-04  train acc  87.61%  test acc  84.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    70  train loss  2.57041e-04  train acc  90.44%  test acc  87.30%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    80  train loss  2.18479e-04  train acc  91.25%  test acc  88.80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    90  train loss  1.98381e-04  train acc  91.85%  test acc  88.40%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   100  train loss  1.77084e-04  train acc  92.32%  test acc  88.60%\n"
     ]
    }
   ],
   "source": [
    "lr = 3e-4\n",
    "weight_decay = 4.5e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 101\n",
    "output_every = 10\n",
    "\n",
    "num_train = len(train_ind)\n",
    "num_test = len(test_ind)\n",
    "\n",
    "for t in range(epochs):\n",
    "    \n",
    "    model.train()\n",
    "    y_hat = model(X_pt)\n",
    "    loss = loss_fn(y_hat[train_ind], y_pt[train_ind])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t % output_every == 0:\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            y_hat = model(X_pt)\n",
    "            train_acc = torch.sum(y_hat[train_ind].max(1).indices == y_pt[train_ind])/num_train\n",
    "            test_acc = torch.sum(y_hat[test_ind].max(1).indices == y_pt[test_ind])/num_test\n",
    "            \n",
    "            print(f'epoch {t:5d}  train loss {loss.item()/num_train: 0.5e}  train acc {100*train_acc:6.2f}%  test acc {100*test_acc:6.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GCN using [PyG](https://www.pyg.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "edge_index = [ (node_map[u], node_map[v]) for u,v in edges]\n",
    "edge_index += [ (node_map[u], node_map[v]) for v,u in edges]\n",
    "edge_index = torch.tensor(edge_index).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "hidden_channels = 256\n",
    "hidden_layers = 4\n",
    "out_channels = len(label_set)\n",
    "\n",
    "class GCN2(nn.Module):\n",
    "    \n",
    "    def __init__(self, edge_index, in_channels, out_channels, hidden_channels, hidden_layers):\n",
    "        \n",
    "        # super-class initializer\n",
    "        super().__init__()\n",
    "        \n",
    "        self.edge_index = edge_index\n",
    "        \n",
    "        # create a module list for the layers\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        # first hidden layer : in_channels -> hidden_channels\n",
    "        self.layers.append(tgnn.GCNConv(in_channels, hidden_channels))\n",
    "        \n",
    "        # remaining hidden layers : hidden_channels -> hidden_channels\n",
    "        for i in range(hidden_layers-1):\n",
    "            self.layers.append(tgnn.GCNConv(hidden_channels, hidden_channels))\n",
    "            \n",
    "        # output layer : hidden_channels -> out_channels\n",
    "        self.layers.append(tgnn.GCNConv(hidden_channels, out_channels))\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        dropout = nn.functional.dropout\n",
    "        \n",
    "        l = self.layers[0]\n",
    "        X = l(X, self.edge_index).relu()\n",
    "        X = dropout(X, 0.75, training=self.training)\n",
    "        \n",
    "        for l in self.layers[1:-1]:\n",
    "            X = X + l(X, self.edge_index).relu()\n",
    "\n",
    "        X = dropout(X, 0.75, training=self.training)\n",
    "        l = self.layers[-1]\n",
    "        return l(X, self.edge_index)\n",
    "    \n",
    "model = GCN2(edge_index, in_channels, out_channels, hidden_channels, hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch     0  train loss  1.32430e-03  train acc  18.18%  test acc  17.50%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    10  train loss  1.11990e-03  train acc  33.33%  test acc  32.40%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    20  train loss  9.32468e-04  train acc  70.10%  test acc  67.80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    30  train loss  6.73592e-04  train acc  81.21%  test acc  79.80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    40  train loss  4.40061e-04  train acc  87.27%  test acc  85.30%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    50  train loss  3.18909e-04  train acc  89.43%  test acc  87.70%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    60  train loss  2.53249e-04  train acc  90.10%  test acc  88.80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    70  train loss  2.29370e-04  train acc  91.58%  test acc  89.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    80  train loss  2.08358e-04  train acc  91.65%  test acc  88.80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    90  train loss  1.92950e-04  train acc  92.53%  test acc  88.90%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   100  train loss  1.70246e-04  train acc  92.46%  test acc  88.90%\n"
     ]
    }
   ],
   "source": [
    "lr = 3e-4\n",
    "weight_decay = 4.5e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "epochs = 101\n",
    "output_every = 10\n",
    "\n",
    "num_train = len(train_ind)\n",
    "num_test = len(test_ind)\n",
    "\n",
    "for t in range(epochs):\n",
    "    \n",
    "    model.train()\n",
    "    y_hat = model(X_pt)\n",
    "    loss = loss_fn(y_hat[train_ind], y_pt[train_ind])\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if t % output_every == 0:\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "        \n",
    "            y_hat = model(X_pt)\n",
    "            train_acc = torch.sum(y_hat[train_ind].max(1).indices == y_pt[train_ind])/num_train\n",
    "            test_acc = torch.sum(y_hat[test_ind].max(1).indices == y_pt[test_ind])/num_test\n",
    "            \n",
    "            print(f'epoch {t:5d}  train loss {loss.item()/num_train: 0.5e}  train acc {100*train_acc:6.2f}%  test acc {100*test_acc:6.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel",
    "--HistoryManager.enabled=False",
    "--matplotlib=inline",
    "-c",
    "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (system-wide)",
   "env": {
   },
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}